from unicodedata import unidata_version
import torch
import torch.nn as nn
from mmcv.cnn import constant_init, kaiming_init, normal_init
from mmcv.runner import load_checkpoint
from mmcv.utils import _BatchNorm
from ...utils import get_root_logger

from ..builder import HEADS
from .base import BaseHead
#from ..skeleton_gcn.tools_cigcn import unit_tcn
from ..gcns import unit_tcn2 as unit_tcn



def conv_init(conv):
    if conv.weight is not None:
        nn.init.kaiming_normal_(conv.weight, mode='fan_out')
    if conv.bias is not None:
        nn.init.constant_(conv.bias, 0)

def bn_init(bn, scale):
    nn.init.constant_(bn.weight, scale)
    nn.init.constant_(bn.bias, 0)


@HEADS.register_module()
class RepVGGHead(BaseHead):
    '''
    input type: tuple, generated by repvgg backbone
    '''
    def __init__(self,
                 num_classes,
                 in_channels,
                 clip_len=1,
                 num_clips=1,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        self.num_clips = num_clips

        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError
            
        self.fc1 = nn.Linear(in_channels, num_classes)

        self.pretrained = pretrained

    def pre_logits(self, x):
        if isinstance(x, tuple):
            x = x[-1]
        return x

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW)
        '''
        # come from mmcls.necks
        if isinstance(inputs, tuple):
            outs = tuple([self.pool(x) for x in inputs])
            outs = tuple([out.view(x.size(0), -1) for out, x in zip(outs, inputs)])
        elif isinstance(inputs, torch.Tensor):
            outs = self.pool(inputs) # (N*M, 512, 1, 1)
            outs = outs.view(inputs.size(0), -1)
        else:
            raise TypeError('inputs should be tuple or torch.tensor')

        # come from mmcls.losses.linear_loss
        outs = self.pre_logits(outs)
        output = self.fc1(outs)

        if self.num_clips != 1:
            output = output.contiguous().view(-1, self.num_clips, self.num_classes)
            output = torch.mean(output[:, :self.num_clips, :], dim=1)

        return output

@HEADS.register_module()
class CIGCNHead(BaseHead):
    """The classification head for CIGCN.

    Args:
        num_classes (int): Number of classes to be classified.
        in_channels (int): Number of channels in input feature.
        loss_cls (dict): Config for building loss.
            Default: dict(type='CrossEntropyLoss')
        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.
        num_person (int): Number of person. Default: 1.
        init_std (float): Std value for Initiation. Default: 0.01.
    """
    def __init__(self,
                 num_classes,
                 in_channels,
                 clip_len=1,
                 num_clips=1,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        self.num_clips = num_clips

        self.tcn = unit_tcn(in_channels, 512, seg=clip_len)


        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError
            
        self.fc1 = nn.Linear(512, num_classes)

        self.pretrained = pretrained

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW) in train or val
            1 C T V in test part if num_clips == 1

        '''        
        ## step 4: classification

        
        #output = self.tcn(inputs) # (N*M, 512, 1, 20)
        output = self.pool(inputs) # (N*M, 512, 1, 1)
        output = torch.flatten(output, start_dim=1, end_dim=-1) # (N*M, 512) 
        output = self.fc1(output)

        if self.num_clips != 1:
            output = output.contiguous().view(-1, self.num_clips, self.num_classes)
            output = torch.mean(output[:, :self.num_clips, :], dim=1)

        return output


@HEADS.register_module()
class CIGCNHeadTCN(BaseHead):
    """The classification head for CIGCN.

    Args:
        num_classes (int): Number of classes to be classified.
        in_channels (int): Number of channels in input feature.
        loss_cls (dict): Config for building loss.
            Default: dict(type='CrossEntropyLoss')
        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.
        num_person (int): Number of person. Default: 1.
        init_std (float): Std value for Initiation. Default: 0.01.
    """
    def __init__(self,
                 num_classes,
                 in_channels,
                 clip_len=1,
                 num_clips=1,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        self.num_clips = num_clips

        self.tcn = unit_tcn(in_channels, 512, seg=clip_len)


        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError
            
        self.fc1 = nn.Linear(512, num_classes)

        self.pretrained = pretrained

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW)
        '''        
        ## step 4: classification        
        output = self.tcn(inputs) # (N*M, 512, 1, 20)
        output = self.pool(output) # (N*M, 512, 1, 1)
        output = torch.flatten(output, start_dim=1, end_dim=-1) # (N*M, 512) 
        output = self.fc1(output)

        if self.num_clips != 1:
            output = output.contiguous().view(-1, self.num_clips, self.num_classes)
            output = torch.mean(output[:, :self.num_clips, :], dim=1)

        return output


class ugcn(nn.Module):
    def __init__(self, in_channels, out_channels, mode='joint'):
        super(ugcn, self).__init__()
        self.mode = mode
        self.residual = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1),
            nn.BatchNorm2d(out_channels)
        )

        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.bn = nn.BatchNorm2d(out_channels)

        self.relu = nn.ReLU()

        conv_init(self.conv1)
        bn_init(self.bn, 1)

    def forward(self, input, A):
        ''' spatial conv? 
        shape:
            input: (N, C, V, T)
            A: (N, C, V, V) or (N, C, T, T)
            output: N C V T
        '''
        if self.mode == 'joint':
            out = torch.matmul(A, input) # (N, C, V, T)
        else:
            A = A.permute(0, 1, 3, 2).contiguous() # (N, C, T, T)
            out = torch.matmul(input, A) # (N, C, V, T)
        
        output = self.bn(self.conv1(out)) + self.residual(input)
        output = self.relu(output)
        return output


# from cigcn.py
class unit_adjacent(nn.Module):
    def __init__(self, in_channels, out_channels=64, mode='joint'):
        super(unit_adjacent, self).__init__()
        self.mode = mode
        self.conv_query = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv_key = nn.Conv2d(in_channels, out_channels, kernel_size=1)

        self.softmax = nn.Softmax(-1)

        conv_init(self.conv_query)
        conv_init(self.conv_key)

    def forward(self, input):
        '''
        shape:
            input: (N, C, V, T)
            output: (N, C, V, V) or NCTT
        '''
        query = self.conv_query(input) # (N, C, V, T)
        key = self.conv_key(input).permute(0, 1, 3, 2).contiguous() # (N, C, T, V)
        if self.mode == 'joint':
            A = torch.matmul(query, key) # (N, C, V, V)
        else:
            A = torch.matmul(key, query) # (N, C, T, T)

        A = self.softmax(A)
        return A

#@HEADS.register_module()
class GCNHead(BaseHead):
    """The classification head for CIGCN.

    Args:
        num_classes (int): Number of classes to be classified.
        in_channels (int): Number of channels in input feature.
        loss_cls (dict): Config for building loss.
            Default: dict(type='CrossEntropyLoss')
        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.
        num_person (int): Number of person. Default: 1.
        init_std (float): Std value for Initiation. Default: 0.01.
    """
    def __init__(self,
                 num_classes,
                 in_channels,
                 clip_len=1,
                 num_clips=1,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.adaA = unit_adjacent(in_channels, in_channels)

        self.gcn1 = ugcn(in_channels, in_channels)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        self.num_clips = num_clips

        self.tcn = unit_tcn(in_channels, 512, seg=clip_len)


        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError
            
        self.fc1 = nn.Linear(512, num_classes)

        self.pretrained = pretrained

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW)
        '''

        # gcn
        adaA = self.adaA(inputs)

        output = self.gcn1(inputs, adaA)

        ## step 4: classification        
        output = self.tcn(inputs) # (N*M, 512, 1, 20)
        output = self.pool(output) # (N*M, 512, 1, 1)
        output = torch.flatten(output, start_dim=1, end_dim=-1) # (N*M, 512) 
        output = self.fc1(output)

        if self.num_clips != 1:
            output = output.contiguous().view(-1, self.num_clips, self.num_classes)
            output = torch.mean(output[:, :self.num_clips, :], dim=1)

        return output

@HEADS.register_module()
class CTRGCNHead(BaseHead):
    """The classification head for CIGCN.

    Args:
        num_classes (int): Number of classes to be classified.
        in_channels (int): Number of channels in input feature.
        loss_cls (dict): Config for building loss.
            Default: dict(type='CrossEntropyLoss')
        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.
        num_person (int): Number of person. Default: 1.
        init_std (float): Std value for Initiation. Default: 0.01.
    """
    def __init__(self,
                 num_classes,
                 in_channels,
                 clip_len=1,
                 num_clips=1,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        self.num_clips = num_clips

        self.tcn = unit_tcn(in_channels, 512, seg=clip_len, mode='TV')

        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError
            
        self.fc1 = nn.Linear(512, num_classes)

        self.pretrained = pretrained

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW)
        '''
        ## step 4: classification
        output = self.tcn(inputs) # (N*M, 512, 20, 1)
        #'''
        output = self.pool(output) # (N*M, 512, 1, 1)
        output = torch.flatten(output, start_dim=1, end_dim=-1) # (N*M, 512) 
        #'''
        output = self.fc1(output)

        if self.num_clips != 1:
            output = output.contiguous().view(-1, self.num_clips, self.num_classes)
            output = torch.mean(output[:, :self.num_clips, :], dim=1)

        return output


@HEADS.register_module()
class CTRGCNReHead(BaseHead):
    """The classification head for CIGCN.

    Args:
        num_classes (int): Number of classes to be classified.
        in_channels (int): Number of channels in input feature.
        loss_cls (dict): Config for building loss.
            Default: dict(type='CrossEntropyLoss')
        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.
        num_person (int): Number of person. Default: 1.
        init_std (float): Std value for Initiation. Default: 0.01.
    """
    def __init__(self,
                 num_classes,
                 in_channels,
                 clip_len=1,
                 num_clips=1,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        self.num_clips = num_clips

        self.tcn = unit_tcn(in_channels, 512, seg=clip_len, mode='TV')

        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError
            
        self.fc1 = nn.Linear(512, num_classes)

        self.pretrained = pretrained

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW)
        '''
        ## step 4: classification
        output = self.tcn(inputs) # (N*M, 512, 20, 1)
        #'''
        output = self.pool(output) # (N*M, 512, 1, 1)
        output = torch.flatten(output, start_dim=1, end_dim=-1) # (N*M, 512) 
        #'''
        output = self.fc1(output)

        if self.num_clips != 1:
            output = output.contiguous().view(-1, self.num_clips, self.num_classes)
            output = torch.mean(output[:, :self.num_clips, :], dim=1)

        return output


@HEADS.register_module()
class GTCNHead(BaseHead):
    """The classification head for CIGCN.

    Args:
        num_classes (int): Number of classes to be classified.
        in_channels (int): Number of channels in input feature.
        loss_cls (dict): Config for building loss.
            Default: dict(type='CrossEntropyLoss')
        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.
        num_person (int): Number of person. Default: 1.
        init_std (float): Std value for Initiation. Default: 0.01.
    """
    def __init__(self,
                 num_classes,
                 in_channels,
                 clip_len=1,
                 num_clips=1,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 tcn=True,
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        self.num_clips = num_clips

        if tcn == True:
            self.tcn = unit_tcn(in_channels, 512, seg=clip_len, mode='TV')
            self.fc1 = nn.Linear(512, num_classes)
        else:
            self.tcn = lambda x: x
            self.fc1 = nn.Linear(in_channels, num_classes)

        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError
        

        self.pretrained = pretrained

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW)
        '''
        ## step 4: classification
        output = self.tcn(inputs) # (N*M, 512, 20, 1)
        #'''
        output = self.pool(output) # (N*M, 512, 1, 1)
        output = torch.flatten(output, start_dim=1, end_dim=-1) # (N*M, 512) 
        #'''
        output = self.fc1(output)

        if self.num_clips != 1:
            output = output.contiguous().view(-1, self.num_clips, self.num_classes)
            output = torch.mean(output[:, :self.num_clips, :], dim=1)

        return output


@HEADS.register_module()
class GTCNHeadDcls(BaseHead):
    """The classification head for CIGCN.

    Args:
        num_classes (int): Number of classes to be classified.
        in_channels (int): Number of channels in input feature.
        loss_cls (dict): Config for building loss.
            Default: dict(type='CrossEntropyLoss')
        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.
        num_person (int): Number of person. Default: 1.
        init_std (float): Std value for Initiation. Default: 0.01.
    """
    def __init__(self,
                 num_classes,
                 in_channels,
                 clip_len=1,
                 num_clips=1,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 tcn=True,
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        self.num_clips = num_clips

        if tcn == True:
            self.tcn = unit_tcn(in_channels, 512, seg=clip_len, mode='TV')
            self.fc1 = nn.Linear(512, num_classes)
        else:
            self.tcn = lambda x: x
            self.fc1 = nn.Linear(in_channels, num_classes)

        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError
        

        self.pretrained = pretrained

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW)
        '''
        ## step 4: classification
        output = self.tcn(inputs) # (N*M, 512, 20, 1)
        #'''
        output = self.pool(output) # (N*M, 512, 1, 1)
        output = torch.flatten(output, start_dim=1, end_dim=-1) # (N*M, 512) 
        #'''
        output = self.fc1(output)

        if self.num_clips != 1:
            output = output.contiguous().view(-1, self.num_clips, self.num_classes)
            output = torch.mean(output[:, :self.num_clips, :], dim=1)


        return output


@HEADS.register_module()
class GTCNHeadCoco(BaseHead):
    """The classification head for CIGCN.

    Args:
        num_classes (int): Number of classes to be classified.
        in_channels (int): Number of channels in input feature.
        loss_cls (dict): Config for building loss.
            Default: dict(type='CrossEntropyLoss')
        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.
        num_person (int): Number of person. Default: 1.
        init_std (float): Std value for Initiation. Default: 0.01.
    """
    def __init__(self,
                 num_classes,
                 in_channels,
                 loss_cls=dict(type='CrossEntropyLoss'),
                 spatial_type='max',
                 pretrained=None):
        super().__init__(num_classes, in_channels, loss_cls)

        self.spatial_type = spatial_type
        self.num_classes = num_classes
        

        
        self.pool = None
        if self.spatial_type == 'avg':
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
        elif self.spatial_type == 'max':
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
        else:
            raise NotImplementedError

        self.fc1 = nn.Linear(in_channels, num_classes)
        

        self.pretrained = pretrained

    def init_weights(self):
        """Initiate the parameters either from existing checkpoint or from
        scratch."""
        if isinstance(self.pretrained, str):
            logger = get_root_logger()
            logger.info(f'load model from: {self.pretrained}')

            load_checkpoint(self, self.pretrained, strict=False, logger=logger)

        elif self.pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    kaiming_init(m)
                elif isinstance(m, nn.Linear):
                    normal_init(m)
                elif isinstance(m, _BatchNorm):
                    constant_init(m, 1)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, inputs):
        '''
        type of inputs: tuple
        shape of inputs:
            N C T V (NCHW) 
            N M C T V for ntu60-hrnet.pkl
        '''
        N, M, C, T, V = inputs.shape # N, M, 512, T, V
        inputs = inputs.reshape(N * M, C, T, V)

        #'''
        output = self.pool(inputs) # (N*M, 512, 1, 1)
        output = torch.flatten(output, start_dim=1, end_dim=-1) # (N*M, 512) 
        #'''
        output = self.fc1(output) # N*M 60

        #if self.num_clips != 1:
        #    output = output.contiguous().view(-1, self.num_clips, self.num_classes)
        #    output = torch.mean(output[:, :self.num_clips, :], dim=1)

        #output = output.contiguous().view(N, M, self.num_classes)
        #output = torch.mean(output[:, :M, :], dim=1)
        output = output.reshape(N, M, self.num_classes)
        output = torch.mean(output[:, :M, :], dim=1)

        return output

    